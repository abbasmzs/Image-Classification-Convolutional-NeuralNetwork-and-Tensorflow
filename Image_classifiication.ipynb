{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Using Tensorflow on Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed_room', 'dining_room', 'living_room']\n",
      "Types of rooms found:  3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dataset_path = os.listdir('dataset')\n",
    "\n",
    "room_types = os.listdir('dataset')\n",
    "print (room_types)  #what kinds of rooms are in this dataset\n",
    "\n",
    "print(\"Types of rooms found: \", len(dataset_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n",
      "[('bed_room', 'dataset/bed_room/bed-1303451__340.jpg')]\n"
     ]
    }
   ],
   "source": [
    "rooms = []\n",
    "\n",
    "for item in room_types:\n",
    " # Get all the file names\n",
    " all_rooms = os.listdir('dataset' + '/' +item)\n",
    " #print(all_shoes)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_rooms:\n",
    "    rooms.append((item, str('dataset' + '/' +item) + '/' + room))\n",
    "    print(rooms[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  room type                                  image\n",
      "0  bed_room  dataset/bed_room/bed-1303451__340.jpg\n",
      "1  bed_room  dataset/bed_room/bed-1846251__340.jpg\n",
      "2  bed_room  dataset/bed_room/bed-3786264__340.jpg\n",
      "3  bed_room  dataset/bed_room/bed-4065946__340.jpg\n",
      "4  bed_room  dataset/bed_room/bed-4343379__340.jpg\n",
      "      room type                                         image\n",
      "42  living_room  dataset/living_room/pexels-photo-245208.jpeg\n",
      "43  living_room  dataset/living_room/pexels-photo-275484.jpeg\n",
      "44  living_room  dataset/living_room/pexels-photo-276583.jpeg\n",
      "45  living_room  dataset/living_room/pexels-photo-276724.jpeg\n",
      "46  living_room  dataset/living_room/pexels-photo-279719.jpeg\n"
     ]
    }
   ],
   "source": [
    "# Build a dataframe        \n",
    "rooms_df = pd.DataFrame(data=rooms, columns=['room type', 'image'])\n",
    "print(rooms_df.head())\n",
    "print(rooms_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rooms in the dataset:  47\n"
     ]
    }
   ],
   "source": [
    "# Let's check how many samples for each category are present\n",
    "print(\"Total number of rooms in the dataset: \", len(rooms_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rooms in each category: \n",
      "living_room    24\n",
      "bed_room       12\n",
      "dining_room    11\n",
      "Name: room type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "room_count = rooms_df['room type'].value_counts()\n",
    "\n",
    "print(\"rooms in each category: \")\n",
    "print(room_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "path = 'dataset/'\n",
    "\n",
    "\n",
    "im_size = 60\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for i in room_types:\n",
    "    data_path = path + str(i)  # entered in 1st folder and then 2nd folder and then 3rd folder\n",
    "    #filenames = [i for i in os.listdir(data_path) if i.endswith('.jpg')]\n",
    "    filenames = [i for i in os.listdir(data_path) ]\n",
    "   # print(filenames)  # will get the names of all images which ends with .jpg extension\n",
    "    for f in filenames:\n",
    "        img = cv2.imread(data_path + '/' + f)  # reading that image as array\n",
    "        #print(img)  # will get the image as an array\n",
    "        img = cv2.resize(img, (im_size, im_size))\n",
    "        images.append(img)\n",
    "        labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 60, 60, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the image array to a numpy type\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 60, 60, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed_room' 'bed_room' 'bed_room' 'bed_room' 'bed_room']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "\n",
    "\n",
    "y=rooms_df['room type'].values\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# for y\n",
    "y_labelencoder = LabelEncoder ()\n",
    "y = y_labelencoder.fit_transform (y)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'categorical_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18516\\2976947813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0monehotencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#Converted  scalar output into vector output where the correct class will be 1 and other will be 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0monehotencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m  \u001b[1;31m#(393, 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'categorical_features'"
     ]
    }
   ],
   "source": [
    "#DO NOT use this for youtube training\n",
    "\n",
    "y=y.reshape(-1,1)\n",
    "onehotencoder = OneHotEncoder(categorical_features=[0])  #Converted  scalar output into vector output where the correct class will be 1 and other will be 0\n",
    "Y= onehotencoder.fit_transform(y)\n",
    "Y.shape  #(393, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 60, 60, 3)\n",
      "(44,)\n",
      "(3, 60, 60, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "images, y = shuffle(images, y, random_state=1)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(images, y, test_size=0.05, random_state=415)\n",
    "\n",
    "#inpect the shape of the training and testing.\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#youtube\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(60,60,3)),\n",
    "    keras.layers.Dense(256, activation=tf.nn.tanh),\n",
    "\n",
    "    keras.layers.Dense(3, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 10800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2765056   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,765,827\n",
      "Trainable params: 2,765,827\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#youtube\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#youtube\n",
    "#compute the model parameters\n",
    "model.compile(optimizer=tf.compat.v1.train.AdamOptimizer(),\n",
    "loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 1s 15ms/step - loss: 2.1207 - accuracy: 0.3864\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 5.8245 - accuracy: 0.4318\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 5.5153 - accuracy: 0.5000\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 5.1955 - accuracy: 0.5682\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 5.0437 - accuracy: 0.6818\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 4.8059 - accuracy: 0.7045\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 4.5908 - accuracy: 0.6818\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.3612 - accuracy: 0.7273\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 4.1492 - accuracy: 0.6818\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 3.8932 - accuracy: 0.7273\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.7363 - accuracy: 0.6136\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 3.5307 - accuracy: 0.6818\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 3.2592 - accuracy: 0.7045\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 3.0364 - accuracy: 0.7273\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.8068 - accuracy: 0.7273\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.5704 - accuracy: 0.7500\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.3414 - accuracy: 0.7273\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 2.1475 - accuracy: 0.7045\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 1.8910 - accuracy: 0.7500\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 1.6748 - accuracy: 0.7500\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.4299 - accuracy: 0.7273\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.2138 - accuracy: 0.7273\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.0296 - accuracy: 0.7500\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.8368 - accuracy: 0.7500\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7036 - accuracy: 0.7273\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6731 - accuracy: 0.7273\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6953 - accuracy: 0.7500\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7423 - accuracy: 0.5000\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7430 - accuracy: 0.5000\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7100 - accuracy: 0.5000\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6576 - accuracy: 0.8182\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6040 - accuracy: 0.8636\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5789 - accuracy: 0.7500\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5779 - accuracy: 0.7500\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5793 - accuracy: 0.7500\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.5807 - accuracy: 0.7500\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.5769 - accuracy: 0.7500\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.5533 - accuracy: 0.7500\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.5370 - accuracy: 0.7500\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5139 - accuracy: 0.7500\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4997 - accuracy: 0.7500\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4962 - accuracy: 0.7727\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4896 - accuracy: 0.9091\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4826 - accuracy: 0.9318\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4788 - accuracy: 0.9318\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4656 - accuracy: 0.9545\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4575 - accuracy: 0.9545\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4537 - accuracy: 0.9773\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4416 - accuracy: 0.9773\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4230 - accuracy: 0.9091\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.4065 - accuracy: 0.8636\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4067 - accuracy: 0.8182\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4028 - accuracy: 0.7727\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3990 - accuracy: 0.7727\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3879 - accuracy: 0.8864\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3716 - accuracy: 0.9318\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3596 - accuracy: 0.9545\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.3540 - accuracy: 0.9545\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3479 - accuracy: 0.9545\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.3354 - accuracy: 0.9773\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.3315 - accuracy: 0.9773\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.3235 - accuracy: 0.9773\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3133 - accuracy: 0.9773\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3088 - accuracy: 0.9773\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3030 - accuracy: 0.9545\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2946 - accuracy: 0.9545\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2894 - accuracy: 0.9545\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2874 - accuracy: 0.9773\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2821 - accuracy: 0.9773\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2752 - accuracy: 0.9545\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2710 - accuracy: 0.9773\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2663 - accuracy: 0.9773\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2613 - accuracy: 0.9773\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2562 - accuracy: 0.9773\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2533 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2499 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2452 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2416 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2372 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2364 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2310 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2430 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2318 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2183 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2161 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2108 - accuracy: 0.9773\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2066 - accuracy: 0.9773\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2003 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1964 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1912 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1901 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1864 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1821 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1808 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1771 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1740 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1706 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1667 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1627 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1617 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1587 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1567 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1550 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1522 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1496 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1474 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1458 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1442 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1430 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1412 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1391 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1370 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1347 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1329 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1311 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1298 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1287 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1278 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1268 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1252 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1233 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1212 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1195 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1179 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1168 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1155 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1141 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1130 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1115 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1102 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1089 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1081 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1071 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1072 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1051 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1039 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1024 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1011 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0999 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0992 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0982 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0971 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0960 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0950 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0941 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0930 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0921 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0911 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0904 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0895 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0887 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0878 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0869 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0860 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0850 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0841 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0835 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0828 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0822 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0813 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0805 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0797 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0789 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0781 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0774 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0767 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0760 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0753 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0746 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0739 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0732 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0726 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0720 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0714 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0707 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0701 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0695 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0689 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0683 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0677 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0670 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0664 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0658 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0652 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0646 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0641 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0636 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0630 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0625 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0620 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0614 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0609 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0604 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0599 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0596 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0590 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0585 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0580 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0576 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0572 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21fcefa5fd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#youtube\n",
    "model.fit(train_x,train_y,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "classes_x=np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94526356, 0.03038296, 0.02435355],\n",
       "       [0.00405805, 0.03581014, 0.96013176],\n",
       "       [0.003981  , 0.03655742, 0.95946157]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take input From USER (youtube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=60x60 at 0x21FD6684430>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_preprocessing.image import load_img\n",
    "#Or\n",
    "from tensorflow.keras.utils import load_img\n",
    "import numpy as np\n",
    "image = load_img('t2.jpg', target_size=(60,60)) #predicting correct\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#youtube\n",
    "#convert the image pixels to numpy arrays\n",
    "image = np.array(image)\n",
    "image.shape\n",
    "\n",
    "#reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 115ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03806981, 0.9146566 , 0.04727365]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicted image\n",
    "yhat = model.predict(image)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x=np.reshape(train_x,(373,10800))\n",
    "#print(train_x.shape)\n",
    "\n",
    "#test_x=np.reshape(test_x,(20,10800))\n",
    "#test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Number of classes, one class for each of 5 flower.\n",
    "num_classes = 3\n",
    "\n",
    "# flattened imge\n",
    "n_input = 10800\n",
    "\n",
    "\n",
    "# architecture hyper-parameter\n",
    "learning_rate = 0.001\n",
    "training_iters = 10\n",
    "batch_size = 16\n",
    "display_step = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of placeholder (?, 60, 60, 3) (?, 3)\n"
     ]
    }
   ],
   "source": [
    "img_size=60\n",
    "num_channels=3\n",
    "#x = tf.placeholder(tf.float32, [None, n_input])\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size,img_size,num_channels])  # None,60,60,3\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes]) # None,3\n",
    "\n",
    "\n",
    "\n",
    "print('Shape of placeholder',x.shape, y_.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Convolutional Neural Network Work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional use- if you want to add bias in conv layer then use this function by calling it in conv_net model\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')   # summation = wx+b, Activation\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'w1': tf.Variable(tf.random_normal([5, 5, 3, 32]),name='w1'),\n",
    "    'w2': tf.Variable(tf.random_normal([5, 5, 32, 64]),name='w2'),\n",
    "    'w3': tf.Variable(tf.random_normal([5, 5, 64, 128]),name='w3'),\n",
    "    'wd1': tf.Variable(tf.random_normal([8 * 8 * 128, 2048]),name='wd1'),  \n",
    "    'wout': tf.Variable(tf.random_normal([2048, num_classes]),name='wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([32]),name='b1'),\n",
    "    'b2': tf.Variable(tf.random_normal([64]),name='b2'),\n",
    "    'b3': tf.Variable(tf.random_normal([128]),name='b3'),\n",
    "    'bd1': tf.Variable(tf.random_normal([2048]),name='bd1'),\n",
    "    'bout': tf.Variable(tf.random_normal([num_classes]),name='bout')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases):\n",
    "        \n",
    "    # reshape input to 60x60x3 size\n",
    "    x = tf.reshape(x, shape=[-1, 60, 60, 3])  \n",
    "    \n",
    "    print(\"###########################################################################\")\n",
    "    print(\"size of x is\")\n",
    "    print(x.shape)\n",
    "    \n",
    "  \n",
    "    conv1 = conv2d(x, weights['w1'], biases['b1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    print(\"###########################################################################\")\n",
    "    print(\"size after 1st conv layer is \")\n",
    "    print(conv1.shape)\n",
    "\n",
    "    \n",
    "    #input is 30*30*32 image\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['w2'], biases['b2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print(\"###########################################################################\")\n",
    "    print(\"size after 2nd conv and pooling layer is\")\n",
    "    print(conv2.shape)\n",
    "    \n",
    "    \n",
    "    ### third conv layer\n",
    "    # input is 15*15*64 image\n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['w3'], biases['b3'])\n",
    "  \n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "    print(\"###########################################################################\")\n",
    "    print(\"size after 3rd conv and pooling layer is\")\n",
    "    print(conv3.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #input is 8*8*128 \n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv3 output to fit fully connected layer input   = 8*8*128 = 8192\n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    print(\"###########################################################################\")\n",
    "    print(\"shape after flattening the image\")\n",
    "    print(fc1)  #8192 is the output\n",
    "    \n",
    "    \n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    print(\"###########################################################################\")\n",
    "    print(\"shape after fully connected layer\")\n",
    "    print(fc1)\n",
    "    \n",
    "    \n",
    "    # Output, class prediction\n",
    "    # finally we multiply the fully connected layer with the weights and add a bias term. \n",
    "    out = tf.add(tf.matmul(fc1, weights['wout']), biases['bout'])\n",
    "    print(\"###########################################################################\")\n",
    "    print(\"Output layer\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################################################\n",
      "size of x is\n",
      "(?, 60, 60, 3)\n",
      "###########################################################################\n",
      "size after 1st conv layer is \n",
      "(?, 30, 30, 32)\n",
      "###########################################################################\n",
      "size after 2nd conv and pooling layer is\n",
      "(?, 15, 15, 64)\n",
      "###########################################################################\n",
      "size after 3rd conv and pooling layer is\n",
      "(?, 8, 8, 128)\n",
      "###########################################################################\n",
      "shape after flattening the image\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 8192), dtype=float32)\n",
      "###########################################################################\n",
      "shape after fully connected layer\n",
      "Tensor(\"Relu_3:0\", shape=(?, 2048), dtype=float32)\n",
      "###########################################################################\n",
      "Output layer\n",
      "Tensor(\"Add_1:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = conv_net(x, weights, biases)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-a0edc4318a64>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0  -  cost:  2710028.0\n",
      "epoch :  1  -  cost:  4099955.0\n",
      "epoch :  2  -  cost:  7131205.5\n",
      "epoch :  3  -  cost:  3349683.8\n",
      "epoch :  4  -  cost:  1997662.8\n",
      "epoch :  5  -  cost:  2295634.2\n",
      "epoch :  6  -  cost:  3463196.5\n",
      "epoch :  7  -  cost:  3493877.8\n",
      "epoch :  8  -  cost:  3155960.5\n",
      "epoch :  9  -  cost:  3308183.0\n"
     ]
    }
   ],
   "source": [
    "cost_history=[]\n",
    "n_epochs =10\n",
    "# the execution\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "train_y=train_y.todense()\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    a, c = sess.run([optimizer, cost], feed_dict={x: train_x, y_: train_y})  #working\n",
    "    cost_history = np.append(cost_history,c)  # working\n",
    "    print('epoch : ', i,  ' - ', 'cost: ', c) #working \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_y=test_y.todense()  #working solution of ValueError: setting an array element with a sequence.\n",
    "#print(test_y)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model,1), tf.argmax(y_,1))   \n",
    "correct_prediction \n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "accuracy\n",
    "\n",
    "\n",
    "# retrun the accuracy on the test set.\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={x: test_x, y_:test_y}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CCC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d8f857dfadabc4e67adbd864d09b537e7c9bc4f949e52451e34ace30ba66f67e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
